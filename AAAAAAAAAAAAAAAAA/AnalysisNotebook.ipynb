{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AAAAAAAAAAAAAAAAAnalysis\n",
    "\n",
    "This notebooks documents the scraping of submissions and comments from the /r/AAAAAAAAAAAAAAAAA subreddit, a subreddit dedicated to.. well.. AAAAAAAAAAAAAAAAA.\n",
    "\n",
    "The notebook is split into the sections:\n",
    "\n",
    "1. Scraping data with the pmaw API\n",
    "2. Cleaning the data to remove uninteresting entries (deleted posts, automoderator comments etc..)\n",
    "3. Plotting of various quantities from the scraped data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scraping\n",
    "\n",
    "Scraping submissions and comments using the pmaw API, mostly following the guidelines here: https://pypi.org/project/pmaw/. Took ~40 minutes to scrape everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from pmaw import PushshiftAPI\n",
    "api = PushshiftAPI()\n",
    "\n",
    "def scrape_submissions(subreddit, output_csv, limit=1000, before=None, after=None, overwrite_csv=False):\n",
    "    \n",
    "    if os.path.exists(output_csv) and not overwrite_csv:\n",
    "        print(\"{} Already exists, not overwriting\".format(output_csv))\n",
    "        return\n",
    "    \n",
    "    if before and after:\n",
    "        submissions = api.search_submissions(subreddit=subreddit, limit=limit, before=before, after=after)\n",
    "    else:\n",
    "        submissions = api.search_submissions(subreddit=subreddit, limit=limit)\n",
    "\n",
    "    submission_df = pd.DataFrame(submissions)\n",
    "    submission_df.to_csv(output_csv, header=True, index=False, columns=list(submission_df.axes[1]))\n",
    "\n",
    "    print(\"\\nSuccesfully Written {} submissions to {}\".format(len(submission_df), output_csv))\n",
    "    \n",
    "    \n",
    "def scrape_comments(subreddit, output_csv, limit=1000, before=None, after=None, overwrite_csv=False):\n",
    "    \n",
    "    if os.path.exists(output_csv) and not overwrite_csv:\n",
    "        print(\"{} Already exists, not overwriting\".format(output_csv))\n",
    "        return\n",
    "    \n",
    "    if before and after:\n",
    "        comments = api.search_comments(subreddit=subreddit, limit=limit, before=before, after=after)\n",
    "    else:\n",
    "        comments = api.search_comments(subreddit=subreddit, limit=limit)\n",
    "\n",
    "    comments_df = pd.DataFrame(comments)\n",
    "    comments_df.to_csv(output_csv, header=True, index=False, columns=list(comments_df.axes[1]))\n",
    "\n",
    "    print(\"\\nSuccesfully Written {} comments to {}\".format(len(comments_df), output_csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_submissions.csv Already exists, not overwriting\n",
      "raw_comments.csv Already exists, not overwriting\n"
     ]
    }
   ],
   "source": [
    "scrape_submissions(subreddit = 'AAAAAAAAAAAAAAAAA', output_csv = 'raw_submissions.csv', limit=None, overwrite_csv=False)\n",
    "scrape_comments(subreddit = 'AAAAAAAAAAAAAAAAA', output_csv = 'raw_comments.csv', limit=None, overwrite_csv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions_df = pd.read_csv('raw_submissions.csv', low_memory=False)\n",
    "comments_df = pd.read_csv('raw_comments.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "The raw data we scraped requires some amount of cleaning to remove entries that don't line up with the spirit of the subreddit and or analysis. These will be deleted posts, automoderator posts, and posts from reddit bots (which usually provide some long description of their purpose in their comments).\n",
    "\n",
    "We will then also filter out urls, subreddit links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Authors\n",
    "\n",
    "Manually looked at the top 500 posters from each dataframe, we can gauge what authors / usernames to filter out. Showing here the top 6 as a preview of what this step looks for.\n",
    "\n",
    "\n",
    "These are  '[deleted]', 'AutoModerator' and a list of bots: 'VredditDownloader', 'SaveVideo', 'RepostSleuthBot', 'SaveThisVIdeo', 'sneakpeekbot', 'AutoCrosspostBot', 'CoolDownBot', 'haikusbot', 'MAGIC_EYE_BOT', 'FuckCoolDownBot2', 'FuckThisShitBot41', 'SaveVideo', 'ClickableLinkBot', 'uwutranslator', 'Screem_Bot', 'morse-bot', 'nwordcountbot, 'phonebatterylevelbot', 'SmileBot-2020'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[deleted]\n",
      "fakedimestesso\n",
      "Mickthebrick1\n",
      "runningawaywithyrmom\n",
      "Lansofl\n",
      "Total-Volume-9387\n",
      "sindjaika\n",
      "Catslifephils\n",
      "hamakaze99\n",
      "AbundanceLifeStyle\n",
      "\n",
      "[deleted]\n",
      "MAGIC_EYE_BOT\n",
      "runningawaywithyrmom\n",
      "Mickthebrick1\n",
      "AutoModerator\n",
      "VredditDownloader\n",
      "fakedimestesso\n",
      "RepostSleuthBot\n",
      "LonelyGameBoi\n",
      "billsanzer\n"
     ]
    }
   ],
   "source": [
    "N_FILTER = 10\n",
    "for v in submissions_df['author'].value_counts()[:N_FILTER].index:\n",
    "    print(v)\n",
    "print(\"\")\n",
    "for v in comments_df['author'].value_counts()[:N_FILTER].index:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, ~ is inverting the .isin mask so we remove posts that match an author in the list\n",
    "remove_authors = ['[deleted]', 'AutoModerator', 'VredditDownloader', 'SaveVideo', 'RepostSleuthBot', 'SaveThisVIdeo', 'sneakpeekbot', 'AutoCrosspostBot', 'CoolDownBot', 'haikusbot', 'MAGIC_EYE_BOT', 'FuckCoolDownBot2', 'FuckThisShitBot41', 'SaveVideo', 'ClickableLinkBot', 'uwutranslator', 'Screem_Bot', 'morse-bot', 'nwordcountbot', 'phonebatterylevelbot', 'SmileBot-2020']\n",
    "submissions_df = submissions_df[~submissions_df['author'].isin(remove_authors)]\n",
    "comments_df = comments_df[~comments_df['author'].isin(remove_authors)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Filtering\n",
    "\n",
    "Here I just apply some filterings to tidy up the data we will be analysing (submission titles and comment bodies). Here removing URLs and subreddit/username links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any NA values in the raw dataframe\n",
    "submissions_df = submissions_df.dropna(subset=['title'])\n",
    "comments_df = comments_df.dropna(subset=['body'])\n",
    "\n",
    "# remove http and www URLS\n",
    "# pattern = r'[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)'\n",
    "# After trialing multiple regex patterns, this ends up being the simplest way to remove the majority of these posts while retaining time efficiency\n",
    "submissions_df = submissions_df[submissions_df['title'].str.contains(\"http\")==False]\n",
    "comments_df = comments_df[comments_df['body'].str.contains(\"http\")==False]\n",
    "\n",
    "# regex finds and removes subreddit /r/ links\n",
    "submissions_df['title'] = submissions_df['title'].replace(r'/r/([^\\s/]+)', '', regex=True)\n",
    "comments_df['body'] = comments_df['body'].replace(r'/r/([^\\s/]+)', '', regex=True)\n",
    "\n",
    "# regex finds and removes username /u/ links\n",
    "submissions_df['title'] = submissions_df['title'].replace(r'/u/([^\\s/]+)', '', regex=True)\n",
    "comments_df['body'] = comments_df['body'].replace(r'/u/([^\\s/]+)', '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally there are some HTML encodings in the comments like &amp which should be decoded to avoid the countvectorizer catching amp as a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_decode(s):\n",
    "    \"\"\"\n",
    "    Returns the ASCII decoded version of the given HTML string. This does\n",
    "    NOT remove normal HTML tags like <p>.\n",
    "    \"\"\"\n",
    "    htmlCodes = (\n",
    "            (\"'\", '&#39;'),\n",
    "            ('\"', '&quot;'),\n",
    "            ('>', '&gt;'),\n",
    "            ('<', '&lt;'),\n",
    "            ('&', '&amp;')\n",
    "        )\n",
    "    for code in htmlCodes:\n",
    "        s = s.replace(code[1], code[0])\n",
    "    return s\n",
    "\n",
    "submissions_df['title'] = submissions_df['title'].apply(lambda x: html_decode(x))\n",
    "comments_df['body'] = comments_df['body'].apply(lambda x: html_decode(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
